import time
total_start_time = time.time()

import array
import csv
import matplotlib
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

from sklvq import GLVQ

matplotlib.rc("xtick", labelsize="small")
matplotlib.rc("ytick", labelsize="small")

# from sklearn.datasets import load_iris
# Contains also the target_names and feature_names, which we will use for the plots.
# iris = load_iris()

# data = iris.data
# labels = iris.target
report_buff = []
p=1
q=4
start_time = time.time()
import loadsomresult as ls
file_location = "./somresult/result_"+str(p)+"_"+str(q)+".csv"
data, labels, data_len = ls.loadsom(file_location)

data_train_ratio = 0.4
data_train_len = int(data_len*data_train_ratio)

data_train = data[:data_train_len]
labels_train = labels[:data_train_len]

data_test = data[data_train_len:]
labels_test = labels[data_train_len:]
# data_test = [[207.0, 6.0, 2500.0, 4.0, 1.0, 13.0]]
data_test_len = len(data_test)


# Sklearn's standardscaler to perform z-transform
# scaler = StandardScaler()
scaler = MinMaxScaler()

# Compute (fit) and apply (transform) z-transform
data_train = scaler.fit_transform(data_train)
data_test = scaler.fit_transform(data_test)
# print(data_test)
# The creation of the model object used to fit the data to.
model = GLVQ(
    distance_type="squared-euclidean",
    activation_type="swish",
    activation_params={"beta": 2},
    solver_type="steepest-gradient-descent",
    solver_params={"max_runs": 20, "step_size": 0.1},
)

# Train the model
# start_time = time.time()
# print("Training data...")
model.fit(data_train, labels_train)
# print("Finished. (%.5f seconds)" % (time.time() - start_time))

# Predict the labels using the trained model
# start_time = time.time()
# print("Predicting...")
predicted_labels = model.predict(data_test)
# print("Finished. (%f seconds)" % (time.time() - start_time))
#predicted_labels = model.predict([[1,2,3,4],[4,3,2,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]])

# print(labels_test)
# print(predicted_labels)

# To get a sense of the training performance we could print the classification report.
# print(classification_report(labels_test, predicted_labels))
# print(classification_report(predicted_labels, labels_test))

# Calculate accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(labels_test, predicted_labels)
# print('Accuracy: %f' % accuracy)

# Calculate F1-Score
from sklearn.metrics import f1_score
f1s = f1_score(labels_test,predicted_labels, average='macro')

report_buff.append([p,q,accuracy,f1s])
print(str(p)+"\t"+str(q)+"\t"+str(accuracy)+"\t"+str(f1s))

from sklearn.metrics import multilabel_confusion_matrix
cm = multilabel_confusion_matrix(labels_test, predicted_labels)

print(cm)

print("Finished. (%s seconds)" % (time.time() - start_time))